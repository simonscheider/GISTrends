---
  title: "Regressions London"
  output: 
  html_document:
  self_contained: false
---
  
<style>
  pre:not([class]) {
    color: white;
    background-color: #272822;
  }
</style>
  

```{r, echo=FALSE, message=FALSE}
library(pacman)

#devtools::install_github("PMassicotte/gtrendsR") # not working
#devtools::install_github('diplodata/gtrendsR') # fix that also doesn't work
pacman::p_load(readr,rvest,urltools,uuid,RSQLite,rjson,rgdal,curl,gtrendsR,dplyr,tmap,rmapshaper)

CUR_DATE = substr(as.character(Sys.time()),0,10)

# Utils ---------------------------------

parse_gt_hits <- function( hits ){
  hits = replace(hits, hits=='<1', .5) # <1 ----> 0.5
  h = as.numeric(hits)
  h[is.na(h)] = 0 # replace NAs with 0
  return(h)
}

```

# Datasets

```{r}

sort_df <- function( df, col, asc=T ){
  sdf = df[ with(df, order(df[,c(col)], decreasing = !asc)), ]
  return(sdf)
}

countries_sdf = readOGR('geodata/world_borders/ne_50m_admin_0_countries_2017/ne_50m_admin_0_countries.shp')
countries_sdf_simpl = rmapshaper::ms_simplify(countries_sdf, keep = .5, keep_shapes = T )
countries_sdf_simpl = spTransform(countries_sdf_simpl, CRS('+proj=robin'))
# +proj=robin +proj=wintri

save_results <- function( df, filename ){
  fdir = paste0('tmp/amsterdam_gtrends_',CUR_DATE,'/')
  dir.create(fdir, showWarnings = FALSE)
  fn = paste0(fdir,filename,'.tsv')
  fnbin = paste0(fdir,filename,'.rds')
  saveRDS( df, file = fnbin, compress=T )
  print(paste('save_results',fn))
  write_tsv( df, fn, append = F, na = '')
}

# create outfolders
dir.create('tmp',showWarnings = F)
#dir.create('tmp/pages',showWarnings = F)

# Load Amsterdam data ---------------------------------
muni_sdf = readRDS( file = 'geodata/netherlands_municipalities_sdf.rds' )
quar_sdf = readRDS( file = 'geodata/netherlands_quarters_sdf.rds' )
neig_sdf = readRDS( file = 'geodata/netherlands_neighbourhoods_sdf.rds' )

# gtrends data
GT_DATA_FOLDER = 'data/amsterdam_gtrends/amsterdam_gtrends_2018-11-03/'

gt_time_df = readRDS( file = file.path(GT_DATA_FOLDER,'interest_over_time_df.rds') )
nrow(gt_time_df)
summary(gt_time_df)
gt_time_df$hits = parse_gt_hits(gt_time_df$hits)
gt_time_df$keyword = as.factor(gt_time_df$keyword)
gt_time_df$GEOUNIT_CODE = as.factor(gt_time_df$GEOUNIT_CODE)
gt_time_df$GEOUNIT_TYPE = as.factor(gt_time_df$GEOUNIT_TYPE)
summary(gt_time_df)
```


# Get Google country labels

```{r}

google_countries = read_tsv('data/google/google_country_names.csv',na = '')
google_countries$ON_GOOGLE = T
nrow(google_countries)
#write_tsv(google_countries,'tmp/google_country_names.tsv', append = F, na = '')

countries_df = countries_sdf@data[,c("ISO_A2","NAME","NAME_LONG")]

m = merge(google_countries, countries_df, by.x = 'GOOGLE_NAME', 
          by.y = 'NAME_LONG', all=T)
m$NAME = NULL
summary(m)
nrow(m)
#View(m)

write_tsv( subset(m,!is.na(m$ISO_A2)), 'tmp/countries_2017_iso_complete.tsv', append = F, na = '')
write_tsv( subset(m,is.na(m$ISO_A2)), 'tmp/countries_2017_iso_missing.tsv', append = F, na = '')

# merge all google labels
missing_df = read_tsv('data/google/countries_2017_iso_missing_manual_entries.tsv')
missing_df = subset(missing_df,missing_df$ISO_A2!='MISSING')

df = subset(m,!is.na(m$ISO_A2))
google_countries_df = rbind(df,missing_df)
google_countries_df = subset(google_countries_df,google_countries_df$ON_GOOGLE==T)
#View(google_countries_df)
google_countries_df$ON_GOOGLE = NULL
saveRDS(google_countries_df, 'data/google/google_country_names_ISO.rds')

rm(m,countries_df,google_countries_df)
```

# Analyse Amsterdam data

```{r}
names(gt_time_df)
#
gt_time_df_summary = gt_time_df %>%
  group_by(GEOUNIT_CODE, QUERY_BASETERM, keyword, GEOUNIT_TYPE) %>%
  summarise(
    n = n(),
    hits_min = min(hits, na.rm = T),
    hits_mean = mean(hits, na.rm = T),
    hits_median = median(hits, na.rm = T),
    hits_max = max(hits, na.rm = T)
  )
gt_time_df_summary$hits_mean = round(gt_time_df_summary$hits_mean,1)
View(gt_time_df_summary)

write_tsv( gt_time_df_summary, 'tmp/gt_time_summary.tsv', append = F, na = '')

```
# Analyse NL global flow data

```{r}

google_countries_df = readRDS('data/google/google_country_names_ISO.rds')

gt_countries = readRDS( 'data/NL_gtrends/NL_gtrends_2018-11-07/nl_amsterdam_countries_over_time_df.rds' )
nrow(gt_countries)
gt_countries$hits = parse_gt_hits(gt_countries$hits)
gt_countries$year = as.factor(gt_countries$year)
gt_countries$keyword = as.factor(gt_countries$keyword)
gt_countries$origin_country = as.factor(gt_countries$location)
gt_countries$location = NULL

# merge with Google labels to get ISO2 country codes
m = merge(gt_countries,google_countries_df,by.x="origin_country",by.y="GOOGLE_NAME",all.x=T)
m$origin_country_iso2 = m$ISO_A2
m$ISO_A2 = NULL
#summary(m)
stopifnot(nrow(m)==nrow(gt_countries))
gt_countries = m
rm(m)
summary(gt_countries)

gt_countries = subset(gt_countries, gt_countries$hits > 0)
gt_countries$hits_log = round(log(gt_countries$hits+1),3)
summary(gt_countries$hits_log)
outdir = 'tmp/NL_global_gtrends_flows/'
unlink(paste0(outdir,'*'))
dir.create(outdir, showWarnings = FALSE)

# Maps by year
for( y in unique(gt_countries$year)){
  for( kw in unique(gt_countries$keyword)){
    df = subset(gt_countries, gt_countries$low_search_volume==T & 
              gt_countries$year == y & gt_countries$keyword == kw )
    mdf = merge(countries_sdf_simpl, df, 
                by.x='ISO_A2', by.y='origin_country_iso2' )
    
    gtrends_map <- tm_shape(mdf) +
      tm_fill(col="hits_log", style="equal", palette="Blues") + #quantile , 
      #tm_scale_bar(width=0.15,position=c("left","bottom")) +   
      #tm_credits("\n\n\n\n\nContains data from Office for National Statistics\nand Ordnance Survey. Crown copyright & database right 2016.",size=0.6,position=c("left","bottom")) + 
      tm_layout(frame=FALSE) + tm_legend(scale=0.75) +
      tm_text(text = "ISO_A2",size = 0.1) +
      tm_borders(col = 'white',lty = .7)
    fn = paste0(outdir,'gtrends_countries_map-',kw,'-',y)
    print(fn)
    save_tmap(gtrends_map,paste0(fn,'.pdf'))
    df = sort_df(df,'hits',F)
    write_tsv(df,paste0(fn,'.tsv'))
    rm(df,mdf,gtrends_map)
  }
}


```
